{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy as sp\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from numpy import loadtxt\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nlp = sp.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.4'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"keywords.txt\", \"r\")\n",
    "data = text_file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excavation: excavation\n",
      "trench: trench\n",
      "slope: slope\n",
      "ditch: ditch\n",
      "backfill: backfill\n",
      "spill: spill\n",
      "sidewall: sidewall\n",
      "dig: dig\n",
      "excavator: excavator\n",
      "caved: cave\n",
      "dirt: dirt\n",
      "soil: soil\n",
      "trough: trough\n",
      "mud: mud\n",
      "incline: incline\n",
      "tilt: tilt\n",
      "descend: descend\n",
      "sink: sink\n",
      "dip: dip\n",
      "rise: rise\n",
      "slant: slant\n",
      "underground: underground\n",
      "dig: dig\n",
      "out: out\n",
      "excavate: excavate\n"
     ]
    }
   ],
   "source": [
    "lemma_keys = open(\"lemma_keys.txt\", \"w\")\n",
    "for i in data:\n",
    "    doc = nlp(i)\n",
    "    for i in doc:\n",
    "        print(i.text + \":\", i.lemma_)\n",
    "        lemma_keys.write(i.lemma_+\"\\n\")\n",
    "lemma_keys.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['excavation', 'trench', 'slope', 'ditch', 'backfill', 'spill', 'sidewall', 'dig', 'excavator', 'caved', 'dirt', 'soil', 'trough', 'mud', 'incline', 'tilt', 'descend', 'sink', 'dip', 'rise', 'slant', 'underground', 'dig out', 'excavate']\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excavation\n",
      "trench\n",
      "slope\n",
      "ditch\n",
      "backfill\n",
      "spill\n",
      "sidewall\n",
      "dig\n",
      "excavator\n",
      "caved\n",
      "dirt\n",
      "soil\n",
      "trough\n",
      "mud\n",
      "incline\n",
      "tilt\n",
      "descend\n",
      "sink\n",
      "dip\n",
      "rise\n",
      "slant\n",
      "underground\n",
      "dig out\n",
      "excavate\n"
     ]
    }
   ],
   "source": [
    "for i in data:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety = pd.read_excel(r\"Overview_and_Count_Report_(HANA)_-_LSR.xlsx\", engine=\"openpyxl\", skiprows=range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Inc. ID  LSR Life Saving Rules - Key  \\\n",
      "0   226505                            1   \n",
      "1   226513                           10   \n",
      "2   226520                           10   \n",
      "3   226523                            2   \n",
      "4   226523                            3   \n",
      "\n",
      "                               LSR Life Saving Rules  \\\n",
      "0               Drive Safely and Without Distraction   \n",
      "1                        No Life Saving rule applies   \n",
      "2                        No Life Saving rule applies   \n",
      "3  Use the Appropriate Personal Protective Equipment   \n",
      "4            Conduct a pre Job Safety Analysis (JSA)   \n",
      "\n",
      "                                      Title BID Workplace Manager (Designate)  \\\n",
      "0        Work: Safe Act Blizzard conditions                   Trevor Georgsen   \n",
      "1                            Work: Safe Act                  Brodie Pattenden   \n",
      "2  Work: Safe Act Black ice on parking lot.                     Jacob Schultz   \n",
      "3               Minor: No Hazard Identified                      Neil Gourley   \n",
      "4               Minor: No Hazard Identified                      Neil Gourley   \n",
      "\n",
      "  BID Incident Entered by BID Incident Reporter BID Contractor  \\\n",
      "0              Scott Orme            Scott Orme   Not assigned   \n",
      "1          Shawn Zanidean        Shawn Zanidean   Not assigned   \n",
      "2           Rodney Watson         Rodney Watson   Not assigned   \n",
      "3          Jason Sneltzer        Jason Sneltzer   Not assigned   \n",
      "4          Jason Sneltzer        Jason Sneltzer   Not assigned   \n",
      "\n",
      "  BID Sub Contractor Occurrence Date  ... BID Is TransCanada Prime on Job?  \\\n",
      "0                  #      2020-01-02  ...                              Yes   \n",
      "1                  #      2020-01-02  ...                              Yes   \n",
      "2                  #      2020-01-02  ...                              Yes   \n",
      "3                  #      2020-01-02  ...                              Yes   \n",
      "4                  #      2020-01-02  ...                              Yes   \n",
      "\n",
      "  BID Prime on Job                                            Summary  \\\n",
      "0     Not assigned  Due to blizzard conditions on my drive to work...   \n",
      "1     Not assigned  Due to heaving, compound man gate was misalign...   \n",
      "2     Not assigned  Walking across the parking lot from one buildi...   \n",
      "3     Not assigned  General communication- while making repairs to...   \n",
      "4     Not assigned  General communication- while making repairs to...   \n",
      "\n",
      "  BID Immediate Action Description BID Resolution Description  \\\n",
      "0                                0     No Resolution Comments   \n",
      "1                                0     No Resolution Comments   \n",
      "2                                0     No Resolution Comments   \n",
      "3                                0     No Resolution Comments   \n",
      "4                                0     No Resolution Comments   \n",
      "\n",
      "  Immediate Action Sentences Immediate Action No Special Characters Sentences  \\\n",
      "0                        [0]                                                0   \n",
      "1                        [0]                                                0   \n",
      "2                        [0]                                                0   \n",
      "3                        [0]                                                0   \n",
      "4                        [0]                                                0   \n",
      "\n",
      "  Immediate Action Removed Stop Words Immediate Action Noun Phrases  \\\n",
      "0                                 [0]                            []   \n",
      "1                                 [0]                            []   \n",
      "2                                 [0]                            []   \n",
      "3                                 [0]                            []   \n",
      "4                                 [0]                            []   \n",
      "\n",
      "  Immediate Action Verbs  \n",
      "0                     []  \n",
      "1                     []  \n",
      "2                     []  \n",
      "3                     []  \n",
      "4                     []  \n",
      "\n",
      "[5 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "print(safety.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Due to blizzard conditions on my drive to work...\n",
      "1    Due to heaving, compound man gate was misalign...\n",
      "2    Walking across the parking lot from one buildi...\n",
      "3    General communication- while making repairs to...\n",
      "4    General communication- while making repairs to...\n",
      "Name: Summary, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(safety['Summary'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to blizzard conditions on my drive to work this morning I decided to turn around and work from home needed to pull over and wait for daylight in order to get back home\n"
     ]
    }
   ],
   "source": [
    "print(safety['Summary'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences = []\n",
    "safety['Immediate Action Sentences'] = \"\"\n",
    "safety['Immediate Action No Special Characters Sentences'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety['BID Immediate Action Description']= safety['BID Immediate Action Description'].replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87430    Gas control started secondary unit, on-call te...\n",
       "87431                                                    0\n",
       "87432                                                    0\n",
       "87433                                                    0\n",
       "87434                                                    0\n",
       "Name: BID Immediate Action Description, dtype: object"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safety['BID Immediate Action Description'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-125-f9d19a37e248>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  safety['Immediate Action Sentences'][index] = '\\n'.join(tokenizer.tokenize(value)).splitlines()\n"
     ]
    }
   ],
   "source": [
    "#Break texts into sentences\n",
    "for index, value in enumerate(safety['BID Immediate Action Description']):\n",
    "    if value :        \n",
    "        safety['Immediate Action Sentences'][index] = '\\n'.join(tokenizer.tokenize(value)).splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0']\n",
      "87435\n"
     ]
    }
   ],
   "source": [
    "print(safety['Immediate Action Sentences'][555])\n",
    "print(len(safety['BID Immediate Action Description']))\n",
    "safety['Immediate Action No Special Characters Sentences'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10    31882\n",
      "3     19804\n",
      "1     17812\n",
      "2      7731\n",
      "8      2802\n",
      "9      2297\n",
      "7      1909\n",
      "6      1865\n",
      "4      1125\n",
      "5       208\n",
      "Name: LSR Life Saving Rules - Key, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(safety['LSR Life Saving Rules - Key'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-128-bef159ca6d12>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  safety['Immediate Action No Special Characters Sentences'][index] = \" \".join(w for w in nltk.wordpunct_tokenize(res) if w.lower() in words or not w.isalpha()) #Remove Non-English words\n"
     ]
    }
   ],
   "source": [
    "#Remove characters/non-English words\n",
    "\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "for index, value in enumerate(safety['Immediate Action Sentences']):\n",
    "    d = ' '.join(value)\n",
    "    res = re.sub(r'[^\\w\\s]', '', d) #Remove punctuations\n",
    "    safety['Immediate Action No Special Characters Sentences'][index] = \" \".join(w for w in nltk.wordpunct_tokenize(res) if w.lower() in words or not w.isalpha()) #Remove Non-English words\n",
    "\n",
    "\n",
    "# print('Original: ', d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'amount', 'per', 'eight', 'itself', 'sometimes', 'should', 'back', 'used', 'within', 'others', 'her', 're', 'up', 'seeming', 'still', 'towards', 'might', 'thereby', 'therefore', 'somehow', 'herself', 'whereupon', '’m', 'have', 'least', 'the', '’s', 'myself', 'because', 'us', 'nothing', 'latter', 'who', 'hers', 'third', \"n't\", 'say', 'using', 'yet', 'besides', 'she', 'formerly', 'too', 'do', 'then', 'what', 'be', 'noone', 'your', 'mostly', 'ourselves', 'whence', 'whole', 'every', 'whereafter', 'again', 'off', 'see', 'such', 'anything', 'nine', 'three', 'otherwise', 'next', 'than', 'amongst', 'will', 'across', 'indeed', 'enough', 'of', 'bottom', 'also', 'wherever', 'really', 'thru', 'therein', 'always', 'has', 'somewhere', 'after', 'meanwhile', 'most', 'some', 'hence', 'hereafter', 'whereby', '‘m', 'latterly', 'take', 'sometime', 'own', 'serious', 'against', 'these', 'whenever', 'anywhere', 'nor', 'without', 'whoever', 'move', 'part', 'nowhere', 'beyond', 'into', 'hereupon', 'may', 'please', 'n‘t', 'everywhere', 'which', 'everything', 'front', 'are', 'him', 'moreover', 'cannot', 'could', '’d', 'whether', '‘ll', 'since', 'is', 'else', 'from', 'made', 'anyway', 'become', 'doing', 'due', 'its', 'one', 'among', 'thus', 'whither', 'above', 'keep', 'thereupon', 'further', 'all', 'seem', 'my', 'each', 'unless', 'when', 'were', 'afterwards', \"'s\", 'a', 'been', 'so', 'empty', 'more', 'can', 'them', 'much', 'ca', 'less', 'it', 'i', 'put', 'although', 'yours', 'rather', 'why', 'if', 'another', 'whose', 'whereas', 'seemed', 'himself', 'does', 'almost', 'to', 'you', 'fifteen', 'never', 'or', 'wherein', '’re', 'make', \"'ve\", 'and', 'perhaps', 'with', 'often', 'first', 'something', 'below', 'how', 'down', 'we', 'being', 'whatever', 'same', 'nevertheless', 'eleven', 'thereafter', 'neither', 'beside', 'our', 'even', 'not', 'seems', 'while', 'former', 'someone', 'am', 'done', 'on', 'only', 'me', 'two', 'that', 'name', 'show', 'six', 'once', 'nobody', 'about', 'none', 'both', 'yourselves', 'anyhow', '‘d', 'where', 'except', 'very', 'became', 'they', 'but', 'mine', 'ever', \"'m\", 'five', 'under', 'out', 'however', 'beforehand', 'top', 'an', 'sixty', 'thence', 'whom', 'during', 'here', 'hundred', 'give', 'namely', 'ten', 'must', 'twelve', 'everyone', 'either', 'behind', 'regarding', 'herein', 'elsewhere', 'no', 'those', 'n’t', 'side', 'themselves', 'was', 'would', 'hereby', 'over', 'now', 'go', \"'re\", 'well', 'call', 'until', 'onto', 'forty', 'fifty', 'this', 'between', 'quite', 'alone', '‘ve', 'any', '‘re', 'upon', 'already', 'toward', 'throughout', 'various', '’ll', 'for', 'there', 'few', 'full', 'ours', 'last', 'becomes', 'just', 'as', 'via', 'his', 'he', 'their', 'in', 'many', 'before', '‘s', 'did', 'had', 'anyone', 'around', 'twenty', 'several', \"'d\", 'other', 'along', 'get', 'at', 'by', 'through', 'becoming', '’ve', 'together', 'though', 'four', \"'ll\", 'yourself'}\n"
     ]
    }
   ],
   "source": [
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data  excavation In/Out:  False\n",
      "Data  trench In/Out:  False\n",
      "Data  slope In/Out:  False\n",
      "Data  ditch In/Out:  False\n",
      "Data  backfill In/Out:  False\n",
      "Data  spill In/Out:  False\n",
      "Data  sidewall In/Out:  False\n",
      "Data  dig In/Out:  False\n",
      "Data  excavator In/Out:  False\n",
      "Data  caved In/Out:  False\n",
      "Data  dirt In/Out:  False\n",
      "Data  soil In/Out:  False\n",
      "Data  trough In/Out:  False\n",
      "Data  mud In/Out:  False\n",
      "Data  incline In/Out:  False\n",
      "Data  tilt In/Out:  False\n",
      "Data  descend In/Out:  False\n",
      "Data  sink In/Out:  False\n",
      "Data  dip In/Out:  False\n",
      "Data  rise In/Out:  False\n",
      "Data  slant In/Out:  False\n",
      "Data  underground In/Out:  False\n",
      "Data  dig out In/Out:  False\n",
      "Data  excavate In/Out:  False\n"
     ]
    }
   ],
   "source": [
    "for i in data:\n",
    "    print(\"Data \", i, \"In/Out: \", i in STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-131-ae27c991207d>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  safety['Immediate Action Removed Stop Words'][index] = [word for word in text_tokens if nlp.vocab[word].is_stop == False] #Remove any word which is a stop word\n"
     ]
    }
   ],
   "source": [
    "safety['Immediate Action Removed Stop Words'] = \"\"\n",
    "for index, value in enumerate(safety['Immediate Action No Special Characters Sentences']):\n",
    "    text_tokens = word_tokenize(value)\n",
    "    safety['Immediate Action Removed Stop Words'][index] = [word for word in text_tokens if nlp.vocab[word].is_stop == False] #Remove any word which is a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['water', 'ice', 'got', 'truck', 'felt', 'hanging', 'wheel', 'truck', 'team', 'lead']\n"
     ]
    }
   ],
   "source": [
    "print(safety['Immediate Action Removed Stop Words'][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-133-aa9e692ae6f0>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  safety['Immediate Action Noun Phrases'][index] = [chunk.text for chunk in doc.noun_chunks]\n",
      "<ipython-input-133-aa9e692ae6f0>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  safety['Immediate Action Verbs'][index] = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n"
     ]
    }
   ],
   "source": [
    "safety['Immediate Action Noun Phrases'] = \" \"\n",
    "safety['Immediate Action Verbs'] = \" \"\n",
    "for index, value in enumerate(safety['Immediate Action Removed Stop Words']):\n",
    "    d = ' '.join(value)\n",
    "    doc = nlp(d)\n",
    "    # Analyze syntax\n",
    "    safety['Immediate Action Noun Phrases'][index] = [chunk.text for chunk in doc.noun_chunks]\n",
    "    safety['Immediate Action Verbs'][index] = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-100-66575c8379d7>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  safety['Summary Nouns'][index] = [token.lemma_ for token in doc if token.pos_ == \"NOUN\"]\n"
     ]
    }
   ],
   "source": [
    "#Get Nouns in the Summary text\n",
    "safety['Summary Nouns'] = \" \"\n",
    "for index, value in enumerate(safety['Summary']):\n",
    "    doc = nlp(value)\n",
    "    # Analyze syntax\n",
    "    safety['Summary Nouns'][index] = [token.lemma_ for token in doc if token.pos_ == \"NOUN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-101-3f49325c97cf>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  safety['Immediate Action Nouns'][index] = [token.lemma_ for token in doc if token.pos_ == \"NOUN\"]\n"
     ]
    }
   ],
   "source": [
    "#Get Nouns in the BID Immediate Action Description text\n",
    "safety['Immediate Action Nouns'] = \" \"\n",
    "for index, value in enumerate(safety['BID Immediate Action Description']):\n",
    "    doc = nlp(value)\n",
    "    # Analyze syntax\n",
    "    safety['Immediate Action Nouns'][index] = [token.lemma_ for token in doc if token.pos_ == \"NOUN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:  [Due to blizzard conditions on my drive to work this morning I decided to turn around and work from home needed to pull over and wait for daylight in order to get back home]\n",
      "Summary nouns  ['blizzard', 'condition', 'drive', 'work', 'morning', 'work', 'home', 'daylight', 'order', 'home']\n",
      "Stop words nouns ['blizzard', 'drive', 'work', 'morning', 'turn', 'work', 'home', 'pull', 'wait', 'daylight', 'order', 'home']\n"
     ]
    }
   ],
   "source": [
    "# # for index, value in enumerate(safety['Removed Stop Words']):\n",
    "# d = ' '.join(safety['Removed Stop Words'][0])\n",
    "# doc = nlp(safety['Summary'][0])\n",
    "# doc2 = nlp(d)\n",
    "# new_sentences = list(doc.sents)\n",
    "# print(\"Original Sentence: \", new_sentences)\n",
    "# nouns = []\n",
    "# # for token in doc:\n",
    "# #     if token.pos_ == \"NOUN\":\n",
    "# #         nouns.append(token)\n",
    "# # print(nouns)\n",
    "# print(\"Summary nouns \", [token.lemma_ for token in doc if token.pos_ == \"NOUN\"])\n",
    "# print(\"Stop words nouns\", [token.lemma_ for token in doc2 if token.pos_ == \"NOUN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety.to_csv(\"new_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Inc. ID', 'LSR Life Saving Rules - Key', 'LSR Life Saving Rules',\n",
       "       'Title', 'BID Workplace Manager (Designate)', 'BID Incident Entered by',\n",
       "       'BID Incident Reporter', 'BID Contractor', 'BID Sub Contractor',\n",
       "       'Occurrence Date', 'BID Discovery Date', 'Reported Date',\n",
       "       'BID Incident Target Due date', 'BID Incident Validation Field',\n",
       "       'Responsible Party', 'Inc. Status', 'BID Work Activity',\n",
       "       'Inc. Org. Unit', 'BID Scorecard Organizational Unit', 'Company',\n",
       "       'Inc. Location', 'Location Description', 'Inc. Region', 'Inc. Country',\n",
       "       'Project #', ' Is this US DOT SRC?', 'Project Definition',\n",
       "       'Actual Severity', 'Incident Category', 'Event', 'Sub Event',\n",
       "       'BID Inc - Recordable Incident', 'BID MCA/MCP',\n",
       "       'BID MCA/MCP Description', 'BID Is TransCanada Prime on Job?',\n",
       "       'BID Prime on Job', 'Summary', 'BID Immediate Action Description',\n",
       "       'BID Resolution Description', 'Sentences',\n",
       "       'No Special Characters Sentences', 'Removed Stop Words', 'Noun Phrases',\n",
       "       'Verbs', 'Immediate Action Sentences',\n",
       "       'Immediate Action No Special Characters Sentences',\n",
       "       'Immediate Action Removed Stop Words', 'Immediate Action Noun Phrases',\n",
       "       'Immediate Action Verbs', 'Summary Nouns', 'Immediate Action Nouns'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safety.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['Inc. ID','LSR Life Saving Rules - Key', 'LSR Life Saving Rules', 'Summary', 'Noun Phrases', 'Verbs','Summary Nouns','BID Immediate Action Description', 'Immediate Action Noun Phrases', 'Immediate Action Verbs', 'Immediate Action Nouns']\n",
    "safety.to_csv('output.csv', columns = header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['decide']\n"
     ]
    }
   ],
   "source": [
    "print(safety['Verbs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"new_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Inc. ID', 'LSR Life Saving Rules - Key',\n",
      "       'LSR Life Saving Rules', 'Title', 'BID Workplace Manager (Designate)',\n",
      "       'BID Incident Entered by', 'BID Incident Reporter', 'BID Contractor',\n",
      "       'BID Sub Contractor', 'Occurrence Date', 'BID Discovery Date',\n",
      "       'Reported Date', 'BID Incident Target Due date',\n",
      "       'BID Incident Validation Field', 'Responsible Party', 'Inc. Status',\n",
      "       'BID Work Activity', 'Inc. Org. Unit',\n",
      "       'BID Scorecard Organizational Unit', 'Company', 'Inc. Location',\n",
      "       'Location Description', 'Inc. Region', 'Inc. Country', 'Project #',\n",
      "       ' Is this US DOT SRC?', 'Project Definition', 'Actual Severity',\n",
      "       'Incident Category', 'Event', 'Sub Event',\n",
      "       'BID Inc - Recordable Incident', 'BID MCA/MCP',\n",
      "       'BID MCA/MCP Description', 'BID Is TransCanada Prime on Job?',\n",
      "       'BID Prime on Job', 'Summary', 'BID Immediate Action Description',\n",
      "       'BID Resolution Description', 'Sentences',\n",
      "       'No Special Characters Sentences', 'Removed Stop Words', 'Noun Phrases',\n",
      "       'Verbs'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safety['BID Immediate Action Description'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.6 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/tensorflow-2.6-cpu-py38-ubuntu20.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
